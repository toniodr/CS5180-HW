{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cceb7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "#Importing some Python libraries\n",
    "# ---------------------------------------------------------\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c88abcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/antonioduran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/antonioduran/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c62c6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemTokenizer:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db7c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Reading the data in a csv file\n",
    "# ---------------------------------------------------------\n",
    "with open('collection.csv', 'r') as csvfile:\n",
    "  reader = csv.reader(csvfile)\n",
    "  for i, row in enumerate(reader):\n",
    "         if i > 0:  # skipping the header\n",
    "            documents.append (row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7403c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love a dog and a cat.', 'She loves her cat and dogs.', 'They love their cat.']\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Print original documents\n",
    "# ---------------------------------------------------------\n",
    "# --> add your Python code here\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b25f7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Instantiate CountVectorizer informing 'word' as the analyzer, Porter stemmer as the tokenizer, stop_words as the identified stop words,\n",
    "# unigrams and bigrams as the ngram_range, and binary representation as the weighting scheme\n",
    "# ---------------------------------------------------------\n",
    "# --> add your Python code here\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer    = 'word',\n",
    "    tokenizer   = StemTokenizer(), \n",
    "    stop_words  = 'english',\n",
    "    ngram_range = (1, 2),\n",
    "    binary      = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aec1f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonioduran/cpp/cs5180 - Information Retrieval/source files/.venv/lib/python3.14/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/antonioduran/cpp/cs5180 - Information Retrieval/source files/.venv/lib/python3.14/site-packages/sklearn/feature_extraction/text.py:411: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Fit the vectorizer to the documents and encode the them\n",
    "# ---------------------------------------------------------\n",
    "# --> add your Python code here\n",
    "\n",
    "vectorizer.fit(documents)\n",
    "document_matrix = vectorizer.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da0c7794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['.', 'cat', 'cat .', 'cat dog', 'dog', 'dog .', 'dog cat', 'love', 'love cat', 'love dog']\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Inspect vocabulary\n",
    "# ---------------------------------------------------------\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39e9d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Fit the vectorizer to the query and encode it\n",
    "# ---------------------------------------------------------\n",
    "# --> add your Python code here\n",
    "\n",
    "query = [\"I love dogs\"]\n",
    "query_vector = vectorizer.transform(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01845adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Convert matrices to plain Python lists\n",
    "# ---------------------------------------------------------\n",
    "# --> add your Python code here\n",
    "\n",
    "doc_vectors = document_matrix.toarray()\n",
    "query_vector = query_vector.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad67fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Compute dot product\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "scores = []\n",
    "# --> add your Python code here\n",
    "for doc_vector in doc_vectors:\n",
    "    score = sum(doc_vector[i] * query_vector[i] for i in range(len(query_vector)))\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb07cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Sort documents by score (descending)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "ranking = []\n",
    "# --> add your Python code here\n",
    "for i, score in enumerate(scores):\n",
    "    ranking.append((i, score))\n",
    "ranking.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84aff293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: I love dogs\n",
      "\n",
      "Document 0: I love a dog and a cat. - Score: 3\n",
      "Document 1: She loves her cat and dogs. - Score: 2\n",
      "Document 2: They love their cat. - Score: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Query:\", query[0])\n",
    "print()\n",
    "for doc_idx, score in ranking:\n",
    "    print(f\"Document {doc_idx}: {documents[doc_idx]} - Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e4cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5180-hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
